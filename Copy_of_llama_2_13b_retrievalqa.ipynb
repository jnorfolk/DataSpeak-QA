{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnorfolk/DataSpeak-QA/blob/main/Copy_of_llama_2_13b_retrievalqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdQvYmlWmNc"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb)\n",
        "\n",
        "# RAG with LLaMa 13B\n",
        "\n",
        "In this notebook we'll explore how we can use the open source **Llama-13b-chat** model in both Hugging Face transformers and LangChain.\n",
        "At the time of writing, you must first request access to Llama 2 models via [this form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) (access is typically granted within a few hours). If you need guidance on getting access please refer to the beginning of this [article](https://www.pinecone.io/learn/llama-2/) or [video](https://youtu.be/6iHVJyX2e50?t=175).\n",
        "\n",
        "---\n",
        "\n",
        "ðŸš¨ _Note that running this on CPU is sloooow. If running on Google Colab you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab._\n",
        "\n",
        "---\n",
        "\n",
        "We start by doing a `pip install` of all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fRq0BSGMBk"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  transformers==4.31.0 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  pinecone-client==2.2.2 \\\n",
        "  datasets==2.14.0 \\\n",
        "  accelerate==0.21.0 \\\n",
        "  einops==0.6.1 \\\n",
        "  langchain==0.0.240 \\\n",
        "  xformers==0.0.20 \\\n",
        "  bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Hugging Face Embedding Pipeline\n",
        "\n",
        "We begin by initializing the embedding pipeline that will handle the transformation of our docs into vector embeddings. We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding."
      ],
      "metadata": {
        "id": "fK7OXFdulxo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 100}\n",
        ")"
      ],
      "metadata": {
        "id": "nQf0ICZXmGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the embedding model to create document embeddings like so:"
      ],
      "metadata": {
        "id": "YSNke_aDnho-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"this is one document\",\n",
        "    \"and another document\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "      f\"a dimensionality of {len(embeddings[0])}.\")"
      ],
      "metadata": {
        "id": "_4uRQacunhDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Vector Index\n",
        "\n",
        "We now need to use the embedding pipeline to build our embeddings and store them in a Pinecone vector index. To begin we'll initialize our index, for this we'll need a [free Pinecone API key](https://app.pinecone.io/)."
      ],
      "metadata": {
        "id": "E4SSLvJqqdhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pinecone\n",
        "\n",
        "# get API key from app.pinecone.io and environment from console\n",
        "# (use your own! - Following the above hyperlink)\n",
        "pinecone.init(\n",
        "    api_key=os.environ.get('PINECONE_API_KEY') or '35f6e3ee-3bf3-444a-a8e6-f9824044188a',\n",
        "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
        ")"
      ],
      "metadata": {
        "id": "lhXARZQXq6QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we initialize the index."
      ],
      "metadata": {
        "id": "MSoNo9uUrlK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Use the name of the index that you created in Pinecone\n",
        "index_name = 'llama-2-rag'\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(embeddings[0]),\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to finish initialization\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "yjs-uPXBrnQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we connect to the index:"
      ],
      "metadata": {
        "id": "5qq5BrajsdEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "nrCwwVQVsfDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our index and embedding process ready we can move onto the indexing process itself. For that, we'll need a dataset. We will use a set of Arxiv papers related to (and including) the Llama 2 research paper."
      ],
      "metadata": {
        "id": "KyckdnprEQDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I will deviate and use my own data."
      ],
      "metadata": {
        "id": "IufZnf9WFM48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can attach your own google drive in this cell block\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LQxAWCoOFFmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_answers = '/content/drive/MyDrive/DataSpeak/Answers.csv'\n",
        "answers = pd.read_csv(path_answers, on_bad_lines='warn', encoding='ISO-8859-1')\n",
        "\n",
        "path_questions = '/content/drive/MyDrive/DataSpeak/Questions.csv'\n",
        "questions = pd.read_csv(path_questions, on_bad_lines='warn', encoding='ISO-8859-1')\n",
        "\n",
        "path_tags = '/content/drive/MyDrive/DataSpeak/Tags.csv'\n",
        "tags = pd.read_csv(path_tags, on_bad_lines='warn', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "2-xT6K5Gpa5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(questions, answers):\n",
        "  df = pd.merge(questions, answers, left_on='Id', right_on='ParentId', how='left', validate='one_to_many', suffixes=['_q', '_a']).dropna()\n",
        "  df = df.drop(['OwnerUserId_q', 'CreationDate_q', 'Id_a', 'OwnerUserId_a', 'CreationDate_a', 'ParentId'], axis=1)\n",
        "  df = df.rename(columns = {'Id_q': 'id', 'Title': 'question_title', 'Body_q': 'question_body', 'Score_a': 'score', 'Body_a': 'answer'})\n",
        "  df = df.drop(df[df.Score_q <= 0].index) # Keep decent questions\n",
        "  df = df.drop(df[df.score <= 0].index) # Keep good answers\n",
        "  idx = df.groupby('id')['score'].head(3).index # Keep top answer\n",
        "  df = df.loc[idx].reset_index(drop=True) # Keep top answer\n",
        "  df = df.sort_values(by=['id', 'score'], ascending=[True, False])\n",
        "  df = df.dropna().reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Mu9pzInIpe6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(df, columns):\n",
        "\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "  def remove_html_tags(text):\n",
        "      soup = BeautifulSoup(text, \"html.parser\")\n",
        "      return soup.get_text()\n",
        "\n",
        "  import re\n",
        "\n",
        "  def remove_chars(text):\n",
        "      # Remove newline characters\n",
        "      text = text.replace('\\n', ' ')\n",
        "\n",
        "      # Remove funny characters\n",
        "      funny_characters = ['\\u2018', '\\u2019', '\\u201C', '\\u201D']\n",
        "      for char in funny_characters:\n",
        "          text = text.replace(char, \"'\")\n",
        "\n",
        "      # Remove any remaining non-ASCII characters\n",
        "      text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "      # Optionally, remove extra spaces\n",
        "      text = ' '.join(text.split())\n",
        "\n",
        "      return text\n",
        "\n",
        "  for column in columns:\n",
        "    df[column] = df[column].apply(remove_html_tags)\n",
        "    df[column] = df[column].apply(remove_chars)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "fQB8j6izpiW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tag_subset(df, tags, tags_list):\n",
        "    df_result = pd.DataFrame()\n",
        "    df_tags = pd.merge(df, tags, how='left', left_on='id', right_on=\"Id\").dropna().drop(['Id', 'Score_q'], axis=1).dropna()\n",
        "    for tag in tags_list:\n",
        "        df_subset = df_tags.loc[df_tags['Tag'].str.contains(tag)].drop('Tag', axis=1)\n",
        "        df_result = pd.concat([df_result, df_subset])\n",
        "        df_result.drop_duplicates(inplace=True)\n",
        "    return df_result"
      ],
      "metadata": {
        "id": "IvO5hRlApsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add any topic that you wish the data to contain to tags_list as a list\n",
        "tags_list = ['numpy', 'pandas', 'django', 'matplotlib']\n",
        "\n",
        "df = preprocess_data(questions, answers)\n",
        "df_cut = get_tag_subset(df, tags, tags_list)\n",
        "data = clean_text(df_cut, columns=['question_title', 'question_body', 'answer'])\n",
        "data = data.drop(data[data.score <= 0].index).drop('Unnamed: 0', axis=1)\n",
        "data['context'] = 'Title: ' + data.question_title + '; Score: ' + str(data.score) + '; Answer: ' + data.answer"
      ],
      "metadata": {
        "id": "6kSxDNlapuU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will embed and index the documents like so:"
      ],
      "metadata": {
        "id": "1KGD2k0rFlkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are embedding and uploading the data to Pinecone. Use your own ids, texts, and metadata if using different data\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data.iloc[i:i_end]\n",
        "    ids = [f\"{x['id']}\" for i, x in batch.iterrows()]\n",
        "    texts = [x['question_title'] for i, x in batch.iterrows()]\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['question_title'],\n",
        "         'context': x['context'],\n",
        "         'source': x['id'],\n",
        "         'answer': x['answer'],\n",
        "         'score': x['score']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ],
      "metadata": {
        "id": "rXSWtOiRFpw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "Z6BhTEMzHvt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQwEeW9Zps2"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElf068NXout"
      },
      "source": [
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-13b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzdi_uMI7B-"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "# Either llama-2 model can be used depending on available processing power\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzX9LqWSX9ot"
      },
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 13B models were trained using the Llama 2 13B tokenizer, which we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0iPv1GDGxgT"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNysQFtPoaj7"
      },
      "source": [
        "Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAYXi8ayKusU"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DG1WNTnJF1o"
      },
      "source": [
        "Confirm this is working:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"How do I do a rolling mean with pandas?\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "fvQtH5YbU3zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhFgmMr0JHUF"
      },
      "outputs": [],
      "source": [
        "# res = generate_text(\"How do I use a ENUM in Django?\")\n",
        "# print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3W3cj3Re1K"
      },
      "source": [
        "Now to implement this in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8RxQYwHRg0N"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiW0_FoQWG6J"
      },
      "outputs": [],
      "source": [
        "# llm(prompt=\"How do I go about specifying and using a ENUM in a Django model?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tv0KxJLvsIa"
      },
      "source": [
        "We still get the same output as we're not really doing anything differently here, but we have now added **Llama 2 13B Chat** to the LangChain library. Using this we can now begin using LangChain's advanced agent tooling, chains, etc, with **Llama 2**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing a RetrievalQA Chain"
      ],
      "metadata": {
        "id": "hVu2KHaMLM2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **R**etrieval **A**ugmented **G**eneration (RAG) in LangChain we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object. For both of these we need an `llm` (which we have initialized) and a Pinecone index â€” but initialized within a LangChain vector store object.\n",
        "\n",
        "Let's begin by initializing the LangChain vector store, we do it like so:"
      ],
      "metadata": {
        "id": "0l9UNP7LLSXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = 'text'  # field in metadata that contains text content\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "oIbTrJDmpddS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm this works like so:"
      ],
      "metadata": {
        "id": "-0dxBmDYpyj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do I do a rolling mean with pandas?\"\n",
        "\n",
        "vectorstore.similarity_search_with_score(\n",
        "    query,  # the search query\n",
        "    k=5  # returns top 5 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "id": "1WhVonePp0hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks good! Now we can put our `vectorstore` and `llm` together to create our RAG pipeline."
      ],
      "metadata": {
        "id": "r3zRCEcUqAGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectorstore.as_retriever(),\n",
        ")"
      ],
      "metadata": {
        "id": "llyEC13RqF9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin asking questions! First let's try *without* RAG:"
      ],
      "metadata": {
        "id": "zjY7R4KDKZTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"how is a series different than a dataframe?\")"
      ],
      "metadata": {
        "id": "PnBrHM1PT7af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, that's not what we meant... What if we use our RAG pipeline?"
      ],
      "metadata": {
        "id": "v33KdwE_Ua6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(\"how is a series different than a dataframe?\")"
      ],
      "metadata": {
        "id": "BEndJT3_KYUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks *much* better! Let's try some more."
      ],
      "metadata": {
        "id": "1QK9mjspUhC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, it looks like the LLM with no RAG is less than ideal â€” let's stop embarassing the poor LLM and stick with RAG + LLM. Let's ask the same question to our RAG pipeline."
      ],
      "metadata": {
        "id": "EPMfjpmhiJui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"what is pandas?\")"
      ],
      "metadata": {
        "id": "hC7hJ-gPoyCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('What is pandas?')"
      ],
      "metadata": {
        "id": "9oR8DzztUli2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reasonable answer from the RAG pipeline, but it doesn't contain much information â€” maybe we can ask more about this, like what is this _\"red team\"_ procedure that delayed the launch of the 34B model?"
      ],
      "metadata": {
        "id": "Z2-I1A3MVZJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('How do I join tables with pandas?')"
      ],
      "metadata": {
        "id": "1GxZJVCBVS8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very interesting!"
      ],
      "metadata": {
        "id": "vpMTOFzKivfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('I have a function. I want to apply it to a column in my pandas dataframe. How do I do this?')\n"
      ],
      "metadata": {
        "id": "5Xg4NQdYiv-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTAFqus3u_Cn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}